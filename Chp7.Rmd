---
title: "Chp7"
author: "Dan Feeney"
date: "4/20/2020"
output: html_document
---

```{r echo = TRUE}
rm(list=ls())
library(rethinking)
sppnames <- c( "afarensis","africanus","habilis","boisei",
"rudolfensis","ergaster","sapiens")
brainvolcc <- c( 438 , 452 , 612, 521, 752, 871, 1350 )
masskg <- c( 37.0 , 35.5 , 34.5 , 41.5 , 55.5 , 61.0 , 53.5 )
d <- data.frame( species=sppnames , brain=brainvolcc , mass=masskg )
summary(d)

d$mass_std <- (d$mass - mean(d$mass))/sd(d$mass) 
d$brain_std <- d$brain / max(d$brain)

```

# Use ridiculous priors where brain mass could be between -1,2. the prior for b is flat and centered on 0
```{r echo = TRUE}
m7.1 <- quap(
alist(
brain_std ~ dnorm( mu , exp(log_sigma) ),
mu <- a + b*mass_std,
a ~ dnorm( 0.5 , 1 ),
b ~ dnorm( 0 , 10 ),
log_sigma ~ dnorm( 0 , 1 )
), data=d )


## you could use OLD to get a posterior distribution for the model, but no distribution for sigma
m7.1_OLS <- lm( brain_std ~ mass_std , data=d )
post <- extract.samples( m7.1_OLS )

## compute R^2 to show why it isn't great. compute a posterior predictive distribution for all observations with sim, subtract each
## from its prediction to get a residual 

set.seed(12)
s <- sim( m7.1 )
r <- apply(s,2,mean) - d$brain_std
resid_var <- var2(r)
outcome_var <- var2( d$brain_std )
1 - resid_var/outcome_var

# function
R2_is_bad <- function( quap_fit ) {
s <- sim( quap_fit , refresh=0 )
r <- apply(s,2,mean) - d$brain_std
1 - var2(r)/var2(d$brain_std)
}

```

# fit a bunch of models with progressively more terms to each model. The last one has an incredibly small SD in the 
## brain ~ dnorm(mu, 0.001). This is because the R^2 is 1 as it passes through each point, so we had to fit a sigma
```{r echo = TRUE}
m7.2 <- quap(
alist(
brain_std ~ dnorm( mu , exp(log_sigma) ),
mu <- a + b[1]*mass_std + b[2]*mass_std^2,
a ~ dnorm( 0.5 , 1 ),
b ~ dnorm( 0 , 10 ),
log_sigma ~ dnorm( 0 , 1 )
), data=d , start=list(b=rep(0,2)) )


m7.3 <- quap(
alist(
brain_std ~ dnorm( mu , exp(log_sigma) ),
mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
b[3]*mass_std^3,
a ~ dnorm( 0.5 , 1 ),
b ~ dnorm( 0 , 10 ),
log_sigma ~ dnorm( 0 , 1 )
), data=d , start=list(b=rep(0,3)) )

m7.4 <- quap(
alist(
brain_std ~ dnorm( mu , exp(log_sigma) ),
mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
b[3]*mass_std^3 + b[4]*mass_std^4,
a ~ dnorm( 0.5 , 1 ),
b ~ dnorm( 0 , 10 ),
log_sigma ~ dnorm( 0 , 1 )
), data=d , start=list(b=rep(0,4)) )

m7.5 <- quap(
alist(
brain_std ~ dnorm( mu , exp(log_sigma) ),
mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
b[3]*mass_std^3 + b[4]*mass_std^4 +
b[5]*mass_std^5,
a ~ dnorm( 0.5 , 1 ),
b ~ dnorm( 0 , 10 ),
log_sigma ~ dnorm( 0 , 1 )
), data=d , start=list(b=rep(0,5)) )

m7.6 <- quap(
alist(
brain_std ~ dnorm( mu , 0.001 ),
mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
b[3]*mass_std^3 + b[4]*mass_std^4 +
  b[5]*mass_std^5 + b[6]*mass_std^6,
a ~ dnorm( 0.5 , 1 ),
b ~ dnorm( 0 , 10 )
), data=d , start=list(b=rep(0,6)) )

```

## determining a penalizing/scoring model of accuracy
## enter: information entropy: the uncertainty in a probability distribution is the average log-probability of an event
###  ( for i events ) -1 * sum(Pi)* log(pi)
```{r echo = TRUE}
#suppose p rain = 0.3 and p sun = 0.7, information entropy is 
p <- c( 0.3 , 0.7 )
-sum( p*log(p) ) #0.61

#in Abu Dhabi p rain 0.1 and sun 0.9 (because its so dry)
p <- c( 0.1 , 0.9 )
-sum( p*log(p) ) #0.3 because it's much easier to predict the weather in Abu Dhabi

#add winter forecasting
p <- c(0.15, 0.25, 0.6)
-sum(p*log(p)) #0.97 Entropy increases because again it is harder to predict 

```
## maximum entropy or maxent finds probability distributions most consistent with states of knowledge. This finds the posterior given a prior. Bayesian updating is maximum entropy 
## From entropy to accuracy: divergence is the additional uncertainty induced by using probabilities from one distribution to describe another (K-L divergence). This is average difference in log probability between target and model.
### cross entropy is when we use a distribution q to describe a parameter p, we calcualte cross entropy
### this is helpful because we want to compare how good two models are. The absolute magnitude does not matter, but their relative magnitudes do. For example two models p and r, This is the models average log probability .
### log probability score is gold standard way to compare predictive accuracy of two models. We use the entire posterior because parameters have distributions and predictions have distributions. We need to take the log of avg probability for each observation i where avg is taken over entire posterior. 
``` {r echo = TRUE}
set.seed(1)
lppd( m7.1 , n=1e4 )
#lppmlog pointwise predictive density. Each value is a log-probability score for a specific observation. (There are only 7 observations). If you sum them, you get total log-probability score for model and data. 
# larger log probability values are better because it is larger accurACY. Deviance is -2 * these scoresso smaller values are better

logprob <- sim( m7.1 , ll=TRUE , n=1e4 ) #calcualtes log probability of each observation with sim wtih ll=TRUE. This returns
# a matrix with a row for each sample and a column for each observation
n <- ncol(logprob)
ns <- nrow(logprob)
f <- function( i ) log_sum_exp( logprob[,i] ) - log(ns)
( lppd <- sapply( 1:n , f ) )

#this still improves with more complicated models
sapply( list(m7.1,m7.2,m7.3,m7.4,m7.5,m7.6) , function(m) sum(lppd(m)) )
#lets do a train/test split to score on the non-observed data
```

## one way to regulate models is to use a 'skeptical prior.' Regularized priors help a lot with small sample sizes. B ~ (0,1) means the model should be skeptical of values outside of -2,2. With a regularized prior, the training deviance may be worse because less of the training data is encoded in the model, but the test deviance may improve. This effect is smaller with larger data sets (e.g. 100 vs 20). 
## Ridge regression are linear models with Gaussian priors for the slope parameters centered at 0. They take an input, lambda, that describes the narrowness of the prior. This is not a Bayesian method, but may be used in lm.ridge in MASS package

# There are two main methods to evaluate models out of sample: cross validation and information criteria
## Cross validation: Divide data into chunks (or folds) and the model predicts at each fold after training on all others and avrage over the score to estimate out-of-sample accuracy. Max # folds is LOOCV leave one out cross validation. Key trouble is if we have 1k observations, we calcualte 1k posteriors. Overcome this by removing folds that are 'unimportant' or they do not affect the posterior much if left out. Unlikely outcomes are more important to train the model: the weight.
### This is the Pereto smoothed importance sampling cross-validation PSIS-LOO
## Other sample is information criteria such as K-L Divergence. The distance between points in deviance is about twice the # parameters in each model. For ordinary linear regressions, the overfitting penalty is 2 x # parameters and is the AIC (Akaine information criteria). AIC = Dtrain + 2p = -2lppd + 2p, where p is # free parameters in the posterior. only reliable with flat priors or are overwhelmed by liklihood and sample size is >> # parameters.
## because flat priors are not usually the best, we use widely applicable information criteria (WAIC). This is the log posterior predictive density and and a penalty ~ variance in posterior
```{r echo = TRUE}
# fit a model and use WAIC
data(cars)
m <- quap(
  alist(
    dist ~ dnorm(mu,sigma),
    mu <- a + b*speed,
    a ~ dnorm(0,100),
    b ~ dnorm(0,10),
    sigma ~ dexp(1)
  ) , data=cars )
set.seed(94)
post <- extract.samples(m,n=1000)
#calculate log-liklihood of each observation i at each sample s from the posterior
n_samples <- 1000
logprob <- sapply( 1:n_samples ,
function(s) {
mu <- post$a[s] + post$b[s]*cars$speed
dnorm( cars$dist , mu , post$sigma[s] , log=TRUE )
} )
#gives a 50 x 1000 matrix with observations in rows and samples in columns 
# compute lppd, average samples in each row, take the log, and add all logs together. Need to do all averaging on the log scale 
# to maintain precision. 
n_cases <- nrow(cars)
lppd <- sapply( 1:n_cases , function(i) log_sum_exp(logprob[i,]) - log(n_samples) )

pWAIC <- sapply( 1:n_cases , function(i) var(logprob[i,]) )
-2*( sum(lppd) - sum(pWAIC) )
```
## WIAC is better at predicting out of sample deviance. CV estimates out of sample deviance as sample size increases, but is worse at small sample sizes. PSIS-LOO approximates CV, so it has the same issue at small sample sizes 
## Summary so far: information divergence is a good measure of accuracy, but we need a penalizing aspect. Flat priors give bad information and regularizing priors will reduce sample fit, but may improve test fit (predictive accuracy). CV, WIAC, and PSIS-LOO let us quantify regularized accuracy

## Counterintuitively, do not just pick model with lowest criterion value and discard the rest. This discards relatively model accuracy in CV/PSIS/WIAC. Instead use model comparison to understand how variables influence prediction 