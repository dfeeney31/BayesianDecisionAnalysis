---
title: "Chp5"
author: "Dan Feeney"
date: "4/16/2020"
output: html_document
---

#Chp 5 statistical rethinking notes
## Spurrious associations
### Multiple regression attempts to 'hold constant' the cofounding variables. Multiple causation explores more of the relation between variables including interactions. The importance of one variable may depend on another
```{r echo = TRUE}
library(rethinking)
rm(list=ls())
data("WaffleDivorce")
d <- WaffleDivorce
# Standardize vars.Scale centers are 0 and makes SD 1
d$A <- scale(d$MedianAgeMarriage)
d$D <- scale(d$Divorce)

sd(d$MedianAgeMarriage) #Think about the prior distributions. A beta of 1 would mean a change of 1 SD for age of marriage is a 1 SD change in divorce rate. Since 1 SD of median marriage age is 1.2, this means every 1.2 years would be associated with a 1 SD rate of divorce, which is huge!
#Since outcome and predictor are standardized to 0, alpha should be near 0
#linear model D ~ normal(ui, sigma), ui <- a _ betaA*Ai, sigma is from an exponential prior
m5.1 <- quap(
    alist(
      D ~ dnorm( mu , sigma ) ,
      mu <- a + bA * A ,
      a ~ dnorm( 0 , 0.2 ) ,
      bA ~ dnorm( 0 , 0.5 ) ,
      sigma ~ dexp( 1 )
) , data = d )

#Simulate from prior with extract.prior and link
set.seed(100)
prior <- extract.prior(m5.1) #sample from the prior and link to plot over the interval [-2,2] (Standard deviation)
mu <- link( m5.1 , post=prior , data=list( A=c(-2,2) ) ) #provides 
plot( NULL , xlim=c(-2,2) , ylim=c(-2,2) )
for ( i in 1:50 ) lines( c(-2,2) , mu[i,] , col=col.alpha("black",0.4) )
#Changing the prior for beta to being more vague or flat results in ridiculous priors
```

## now sample from the posterior with making a sequence of x variables, use link, calcualte mean and PI, and plot the results
```{r echo = TRUE}
# compute percentile interval of mean 
A_seq <- seq( from=-3 , to=3.2 , length.out=30 ) #make a list of values from -3 to 3.2 
mu <- link( m5.1 , data=list(A=A_seq) ) #link to that list and the model to map the parameters
mu.mean <- apply( mu , 2, mean ) #calcualte the mean over columns 
mu.PI <- apply( mu , 2 , PI ) #calculate the PI over columns 
# plot it all
plot( D ~ A , data=d , col=rangi2 )
lines( A_seq , mu.mean , lwd=2 )
shade( mu.PI , A_seq )

```
## relation between divorce rate and marriage rate
```{r echo = TRUE}
d$M <- scale( d$Marriage ) #scale marriage rate
m5.2 <- quap(
    alist(
      D ~ dnorm( mu , sigma ) ,
      mu <- a + bM * M , #saving bM for beta for marriage
      a ~ dnorm( 0 , 0.2 ) ,
      bM ~ dnorm( 0 , 0.5 ) ,
      sigma ~ dexp( 1 )
    ) , data = d )
precis(m5.2)
#use A_seq from previous 
A_seq <- seq(from = -3, to = 3.2, length.out = 30)
mu <- link( m5.2 , data=list(A=A_seq) )
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

plot(D ~ M, data = d, col=rangi2)
lines(A_seq,mu.mean, lwd = 2)
shade(mu.PI, A_seq)
```
## DAG (directed acyclic graph). Directed --> causal influence between variables, acyclic means causes do not eventually flow back on themselves, graph means it is nodes and edges (connections). 
### to construct a DAG between age, marriage rate, and divorce rate, use daggity library. If Y is not associated with X after conditioning on Z, Y ⊥⊥ X|Z.
### D⊥̸⊥ A,D⊥̸⊥ M, A⊥̸⊥ M means they are notindepndent of each other


```{r echo = TRUE}
library(dagitty)
dag5.1 <- dagitty( "dag {
A -> D
A -> M
M -> D
}")
coordinates(dag5.1) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) )
drawdag( dag5.1 )
# or DAG two with no causation between marriage rate and divorce rate
dag5.2 <- dagitty( "dag {
A -> D
A -> M
}")
coordinates(dag5.2) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) )
drawdag( dag5.2 )

DMA_dag2 <- dagitty('dag{ D <- A -> M }')
impliedConditionalIndependencies( DMA_dag2 ) #for the second DAG, these are the conditional independencies. This means all vars are correlated initially, but D and M should be independent after conditioning on A
# DAG 1
DMA_dag1 <- dagitty('dag{ D <- A -> M -> D }')
impliedConditionalIndependencies( DMA_dag1 ) #no output because it implies all variables are independent 
```

## Multiple regression. Mu = alpha + BETAm*M + BETAa*A + sigma where BETAa is beta for age and BETAm beta for marriage rate
### divorce rate can dependent on age of marriage or marriage rate with 'or' representing independent associations
### M = Xb where m is a vector of predicted means (one for each row in data), b is column vector of parameters (one for each predictor), and X is a design matrix. It has as many rows as data and columns as predictors plus 1. X is a dataframe plus a first column filled with 1s, which are multiplied by the first parameter (alpha, or the intercept). When X is matrix multiplied by b, you get predicted means X%*% b in R notation
### now let's approximate the posterior 

```{r echo = TRUE}
m5.3 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a * bM*M + bA*A,
    a ~ dnorm(0,0.2),
    bM ~ dnorm(0,0.5),
    bA ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ), data = d
)

precis( m5.3 ) # mean estimate for bA is still negative and both sides of interval are negative. bM is near 0 with a wide interval
plot( coeftab(m5.1,m5.2,m5.3), par=c("bA","bM") ) #Shows each parameter estimate from the three models (age only, marriage rate only, and both)
#ONce we know the age of marriage for a state, there is little additonal knowledge to be gainted by knowing the marriage rate, indicating       D ⊥⊥ M|A. So the first DAG with causation between A/M and D is not plausible
```

## There is no additional gain in knowing marriage rate once you know age of marraige for a state. There is still value in knowing marriage rate if you cannot find the age of marriage for a state. There is no causal relation between marriage rate and divorce rate or it is spurrious (caused by the effect of age on both marriage and divorce rate)
### plotting helps interprete the results of multiple regression. 1) predictor residual plots show outcome against predictor residuals, 2) posterior prediction plots model-based predictors against raw data (error in prediction), and 3) counterfactual plots show predicted for imaginary examples
### predictor residual plots: M (marriage rate) and A (median age of marriage), to compute residuals, use the other predictor to model it, so for marriage rate (M) predict with A. Then we compute residuals by subtracting observed marriage rate in each state by the predicted one
```{r echo = TRUE}
m5.4 <- quap(
    alist(
      M ~ dnorm( mu , sigma ) ,
      mu <- a + bAM * A ,
      a ~ dnorm( 0 , 0.2 ) ,
      bAM ~ dnorm( 0 , 0.5 ) ,
      sigma ~ dexp( 1 )
    ) , data = d )
mu <- link(m5.4)
mu_mean <- apply( mu , 2 , mean )
mu_resid <- d$M - mu_mean

```

``` {r echo = TRUE}
# call link without specifying new data
# so it uses original data to do a posterior predictive test
mu <- link( m5.3 )
# summarize samples across cases
mu_mean <- apply( mu , 2 , mean )
mu_PI <- apply( mu , 2 , PI )
# simulate observations
# again no new data, so uses original data
D_sim <- sim( m5.3 , n=1e4 )
D_PI <- apply( D_sim , 2 , PI )

plot( mu_mean ~ d$D , col=rangi2 , ylim=range(mu_PI) ,
xlab="Observed divorce" , ylab="Predicted divorce" )
abline( a=0 , b=1 , lty=2 )
for ( i in 1:nrow(d) ) lines( rep(d$D[i],2) , mu_PI[,i] , col=rangi2 )
identify( x=d$D , y=mu_mean , labels=d$Loc )
```

##  counterfactual plots plot any combo of the predictors that you want and regress against them.
### need to demonstrate how each variable influences each other (A -> M for example), so we fit bM*M and bA*A 
### we will simulate values of M and D in that order to check the influence of A on M before estimating the joint influence on D
``` {r echo = TRUE}
data(WaffleDivorce)
d <- list()
d$A <- standardize( WaffleDivorce$MedianAgeMarriage )
d$D <- standardize( WaffleDivorce$Divorce )
d$M <- standardize( WaffleDivorce$Marriage )
m5.3_A <- quap(
    alist(
## A -> D <- M
      D ~ dnorm( mu , sigma ) ,
      mu <- a + bM*M + bA*A ,
      a ~ dnorm( 0 , 0.2 ) ,
      bM ~ dnorm( 0 , 0.5 ) ,
      bA ~ dnorm( 0 , 0.5 ) ,
      sigma ~ dexp( 1 ),
## A -> M
      M ~ dnorm( mu_M , sigma_M ),
      mu_M <- aM + bAM*A,
      aM ~ dnorm( 0 , 0.2 ),
      bAM ~ dnorm( 0 , 0.5 ),
      sigma_M ~ dexp( 1 )
  ) , data = d )

precis(m5.3_A) #bAM is strongly negative, so we could interprete this as if we reduce A, we reduce M

A_seq <- seq( from=-2 , to=2 , length.out=30 ) #Z scores [-2, 2]
sim_dat <- data.frame( A=A_seq )
s <- sim( m5.3_A , data=sim_dat , vars=c("M","D") )

# display counterfactual predictions
plot( sim_dat$A , colMeans(s$D) , ylim=c(-2,2) , type="l" ,
xlab="manipulated A" , ylab="counterfactual D" )
shade( apply(s$D,2,PI) , sim_dat$A )
mtext( "Total counterfactual effect of A on D" )
```


## 5.3 Categorical Variables
### Dummy/indicator variables are encoded to bring unordered categories into quantative models
### using 0/1 for female/male is default, but can be hard to set prior for alpha (what is mean female height?) and it implies there is more uncertainty about male height since it includes both alpha and beta parameters, this may be seen by simulating from the prior for male and females
### Another approach is to use an index approach (1,2,3,...,n) for dummy vars with command d$sex <- ifelse( d$male==1 , 2 , 1 )
```{r echo = TRUE}
data("Howell1")
d <- Howell1

mu_female <- rnorm(1e4,178,20)
mu_male <- rnorm(1e4,178,20) + rnorm(1e4,0,10)
precis( data.frame( mu_female , mu_male ) )

d$sex <- ifelse( d$male==1 , 2 , 1 ) #1 female and 2 male
# approximate the posterior using index variables using a[sex] to indicate there is a factor variable there and the model will estimate for each
m5.8 <- quap( 
    alist(
      height ~ dnorm( mu , sigma ) ,
      mu <- a[sex] ,
      a[sex] ~ dnorm( 178 , 20 ) ,
      sigma ~ dunif( 0 , 50 )
    ) , data=d )
precis( m5.8 , depth=2 ) #shows vectors of parameters (since a is a vector of length 2)

#what is expected difference between males and females? extract samples from posterior and calc the difference. This is a contrast
post <- extract.samples(m5.8)
post$diff_fm <- post$a[,1] - post$a[,2]
precis( post , depth=2 )
```

## now include multiple categories
```{r echo = TRUE}
data("milk")
d <- milk
#coerce to integer for index variables
d$clade_id <- as.integer( d$clade )

d$K <- scale( d$kcal.per.g )

m5.9 <- quap(
  alist(
    K ~ dnorm(mu, sigma),
    mu <- a[clade_id],
    a[clade_id] ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ), data = d
)

labels <- paste( "a[" , 1:4 , "]:" , levels(d$clade) , sep="" )
plot( precis( m5.9 , depth=2 , pars="a" ) , labels=labels ,
xlab="expected kcal (std)" )

## adding in another category
set.seed(63)
d$house <- sample( rep(1:4,each=8) , size=nrow(d) )

m5.10 <- quap(
    alist(
      K ~ dnorm( mu , sigma ),
      mu <- a[clade_id] + h[house],
      a[clade_id] ~ dnorm( 0 , 0.5 ),
      h[house] ~ dnorm( 0 , 0.5 ),
      sigma ~ dexp( 1 )
  ) , data=d )
precis(m5.10, depth = 4)
```