---
title: "Chp4"
author: "Dan Feeney"
date: "4/15/2020"
output: html_document
---

## Chapter 4 notes
## Textbook notes

```{r echo = TRUE}
library(rethinking)
rm(list=ls())

pos <- replicate(1000, sum(runif(16,-1,1))) #1000 replicates of the sum 0f 16 random draws from -1 to 1
plot(density(pos))

log.big <- replicate( 10000 , log(prod(1 + runif(12,0,0.5))) )
plot(density(log.big))

```
## General modeling notes: 1) We have things we want to understand. observable things = data, unobservable things like rates are parameters
### we will define variables in terms of other variables or a distribution. The combo of variables and prob distributions define a joint generative model
### First line (e.g. W ~ Binomial(N,p); p ~ uniform(0,1) defines liklihood function used in Bayes theorem and others define priors. We map variables or parameters onto distributions, meaning they are probabilistic (some values are more plausible than others).

# reminder W ~ Binomial(N,p) W is distributed as a binomial dist with n trials and p prob of success.
## p ~ uniform(0,1) where the prob of success is uniform over the region [0,1] or a noninformative prior. Hi ~N(Mu,Sigma)
```{r echo = TRUE}
data("Howell1")
d <- Howell1
precis(d)
d2 <- subset(d, d$age > 18)

#Take a look at some priors for height
curve( dnorm( x , 178 , 20 ) , from=100 , to=250 ) #height prior
curve( dunif( x , 0 , 50 ) , from=-10 , to=60 ) #SD of height (sigma) prior. This is a flat prior but stays positive between 0 and 50
```

### The prior distributions of height and sigma allow for prior predictive simulation, these imply a joint prior distribution of heights
### hi ~ Normal(mean, sigma). H is normally distributed with mean and standard deviation sigma
### mu ~ Normal(178, 20) (dnorm(178,20))
### sigma ~ Uniform(0,50) (dunif(0,50))

# Prior predictive distribution. We use this to build good priors before building the model and explore the joint prior distribution
### sample from the prior to see the predictions of heights relative combo of parameters provide.
```{r echo = TRUE}
sample_mu <- rnorm(1e4, 178,20)
sample_sigma <- runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma) #random uniform distributed numbers with mean mu and sd sigma
dens(prior_h)
```

## Likely not the best prior because there are some unreasonably tall people in the prior. But at least there are possible values
### really bad prior may have negative people like below. This prior includes a very large SD, which is common in Bayesian methods
### but is not a great idea since it has 4% below to have neg height
```{r echo = TRUE}
sample_mu2 <- rnorm(1e4, 178,100)
sample_sigma2 <- runif(1e4, 0, 50)
prior_h2 <- rnorm(1e4, sample_mu2, sample_sigma2)
dens(prior_h2)
#length(prior_h2[prior_h2 < 0])/ length(prior_h2)
```
## Use grid approximation to calculate the posterior 
```{r echo = TRUE}
mu.list <- seq( from=150, to=160 , length.out=100 ) # simply space values from 150 to 160 evenly for 100 values
sigma.list <- seq( from=7 , to=9 , length.out=100 ) #Same for sigma but from 7 to 9
post <- expand.grid( mu=mu.list , sigma=sigma.list ) #Created a dataframe for the combination of variables (height and sd)
post$LL <- sapply( 1:nrow(post) , function(i) sum(  # sapply returns a vector where we sum the dnorm of height with values of mean and sigma
  dnorm( d2$height , post$mu[i] , post$sigma[i] , log=TRUE ) ) )
post$prod <- post$LL + dnorm( post$mu , 178 , 20 , TRUE ) +
  dunif( post$sigma , 0 , 50 , TRUE )
post$prob <- exp( post$prod - max(post$prod) )
contour_xyz( post$mu , post$sigma , post$prob )
image_xyz( post$mu , post$sigma , post$prob )
```
## now we want to randomly sample rows (since there are multiple parameters) in proportion to probability in post$prob.
## you get 1e4 samples w/ replacement from posterior with height data. Sample row numbers in proportion to posterior probability 
```{r echo = TRUE}
sample.rows <- sample( 1:nrow(post) , size=1e4 , replace=TRUE ,
prob=post$prob ) #grabs rows (num rows to choose from), size, and with replacement. Sample with probability 
sample.mu <- post$mu[ sample.rows ]
sample.sigma <- post$sigma[ sample.rows ]
plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )
# describe shape of the marginal posterior (both mu and sigma), marginal meaning avraging over other parameters
dens(sample.mu)
dens(sample.sigma)
PI(sample.mu)
PI(sample.sigma)
```


## Using quap. Give priors to each parameter. using quadratic approx, the posterior's peak is at maximum a posteriori estimate (MAP).
## quap will return the marginal distribution for each parameter. This is the plausibility for each value of each parameter (e.g. mu), after averaging over the plausibilities of the other parameter (e.g. sigma in this case)
```{r echo = TRUE}
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178,20),
  sigma ~ dunif(0,50)
)

m4.1 <- quap(flist, data = d2)
precis(m4.1)

# you can initialize quap like below. Just to initialize what combo of parameters the algorithm begins with
start <- list(
mu=mean(d2$height),
sigma=sd(d2$height)
)
m4.1 <- quap( flist , data=d2 , start=start )
precis(m4.1)
```
## Changing the prior to being very narrow (sd of 0.1). In this case, the estimate for mu has barely moved off the prior. Sigma also
## changes because you specify the mean must be near 178, so it adjusts sigma accordingly
```{r echo = TRUE}
m4.2 <- quap(
  flist <- alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(178,0.1),
    sigma ~ dunif(0, 50)
  ), data = d2
)
precis(m4.2)
```
## Sampling from Quap: quadratic approx of a posterior distribution with > 1 parameter (mu and sigma) is a multidimensional Gaussian distribution. So, R calcualtes SD for all params and a covariance matrix among pairs of params. Just like mean ans sd describe a 1-D Gaussian, means and a matrix of variances and covariances describe a multidimensional Gaussian.
## vcov returns the variance-covariance matrix. This tells us about relations between parameters & may be decomposed into a vector of variances for parameters and correlation matrix between parameters. Diag will return the variances (sd^2) and cov2cor the correlation matrix
```{r echo = TRUE}
vcov(m4.1)

diag( vcov( m4.1 ) ) 
cov2cor( vcov( m4.1 ) )
```
## now we want to sample from the multidimensional Gaussian posterior, so we sample vectors and maintain the covariance between mu and sigma
## instead of sampling single values, we are sampling vectors of values (mean and sigma in this case).
### This is the key to prob modeling. Construct a prior, calcualte posterior, sample from posterior, update prior
``` {r echo = TRUE}
post <- extract.samples(m4.1, n = 1e4)
head(post)
precis(post)
```

# Now, consider the regression between height and weight
## Use a linear model of height with an additive effect of weight. A parameter (beta) will tell us this relation. For each combo of parameter values (height, beta, etc.), we compute the posterior probability of the plausibility of that combo, given the data. 

### now h and MU have indices so MU is defined differently for each person
### hi ~ Normal(MUi, sigma)
### MUi = alpha + beta(xi - xbar)(deterministic with = rather than ~ probabilistic). beta ~ normal(0,10) is the beta prior. alpha ~ Uniform(0,50).
## This asks the model two questoins, what is height when xi = xbar (or what is alpha) and what is the expected change in height when x moves one unit from xbar (beta)? 

### in the first case with the prior for beta centered on 0, the priors are terrible and give a bunch of unrealistic values (heights below 0). We know this from simulating from the prior
```{r echo = TRUE}
#plot(d2$height, d2$weight)
#Simulate 100 lines based on the prior for alpha and beta
N <- 100
a <- rnorm(N, 178, 20)
b <- rnorm(N, 0, 10)

plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) ,
xlab="weight" , ylab="height" )
abline( h=0 , lty=2 )
abline( h=272 , lty=1 , lwd=0.5 )
mtext( "b ~ dnorm(0,10)" )
xbar <- mean(d2$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) ,
from=min(d2$weight) , to=max(d2$weight) , add=TRUE ,
col=col.alpha("black",0.2) )
```

## Instead of a beta (slope) centered on 0, use the log-normal distribution- rlnorm in R- to ensure positive slopes
```{r echo = TRUE}
## another idea is to use a log-normal for beta, which is encouraged so slopes are all positive
b <- rlnorm(1e4, 0, 1) #random log normal from 0,1 (essentially 0 to ~10)
# a is carried from before and b is constructed here. below, we sample from a and b in the for loop to construct N lines
dens(b)
plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) ,
xlab="weight" , ylab="height" )
abline( h=0 , lty=2 )
abline( h=272 , lty=1 , lwd=0.5 )
mtext( "b ~ dnorm(0,10)" )
xbar <- mean(d2$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) ,
from=min(d2$weight) , to=max(d2$weight) , add=TRUE ,
col=col.alpha("black",0.2) )
```

## Now, to construct the posterior distribution of parameters using Quap and use the prior lognormal(0,1) for the slope (beta)
``` {r echo = TRUE}
#optional, define average weight
xbar <- mean(d2$weight)

lin.mod <- quap(
  flist <- alist(
    height ~ dnorm(mu, sigma), #general formula we expect, height depends on an unknown mu and sigma
    mu <- a + b*(weight- xbar), #define the linear model alpha * beta(diff_in_weight)
    a ~ dnorm(178,20), #the alpha (intercept) prior from a normal distribution with mean 178, sigma 20
    b ~ dlnorm(0,1), #beta is from a log-normal distribution between 0 and 1
    sigma ~ dunif(0,50) #uniform distribution (flat prior) for sigma
  ), data = d2
)
```
### This could also have been fit by encoding b as log_b ~ dnorm(0,1) and then beta = exp(log(beta)) or beta <- exp(log(beta))
### plotting simulations and making tables of the posterior. mean of 0.9 for param b means for 1kg heaver than mean, person is 0.9 cm taller and 89% of posterior density lives between 0.84 and 0.97. 
```{r echo = TRUE}
precis(lin.mod)
round(vcov(lin.mod),3)
pairs(lin.mod) #plots the marginal posteriors and covariances. Little covariance btwn params -> centering
```
## we see the estimate of the average height (alpha) is 154 with a 90% interval of 154.2 to 155.1, slope 0.9 with interval 0.84 to 0.97 and sigma 5.11 between 4.8 and 5.42
## plotting simulations. We use the max a posteriori estimate (MAP) of alpha and beta from the posterior to plot the regression line. This is just one of infinitely many lines from the posterior. 
```{r echo = TRUE}
plot( height ~ weight , data=d2 , col=rangi2 ) #plot raw data
post <- extract.samples(lin.mod) #get samples of posterior IN PROPORTION TO THEIR PLAUSIBILITY
a_map <- mean(post$a) #maximum a posteriori estimate
b_map <- mean(post$b) #MAP
curve( a_map + b_map*(x - xbar) , add=TRUE ) #take MAPs for a and b and make a curve based on them

## 
head(post)
# each row is a correlated random sample from the joint posterior of all three parameters using covariances from vcov(lin.mod)
# The mean of these is what we plotted above and the spread gives an indication of uncertainty
```
## Estimate from only the first n cases and estimate the model and plot 20 lines from the joint posterior. As n grows, the lines are more compact
```{r echo = TRUE}
N <- 30
dN <- d2[ 1:N , ]
mN <- quap(
  alist(
      height ~ dnorm( mu , sigma ) ,
      mu <- a + b*( weight - mean(weight) ) ,
      a ~ dnorm( 178 , 20 ) ,
      b ~ dlnorm( 0 , 1 ) ,
      sigma ~ dunif( 0 , 50 )
    ) , data=dN )

# extract 20 samples from the posterior
post <- extract.samples( mN , n=20 )
# display raw data and sample size
plot( dN$weight , dN$height ,
xlim=range(d2$weight) , ylim=range(d2$height) ,
col=rangi2 , xlab="weight" , ylab="height" )
mtext(concat("N = ",N))
# plot the lines, with transparency
for ( i in 1:20 )
curve( post$a[i] + post$b[i]*(x-mean(dN$weight)) ,
col=col.alpha("black",0.3) , add=TRUE )
```

```{r echo = TRUE}
#extract samples and compare for a mean of 50 kg
post <- extract.samples(lin.mod)
mu_at_50 <- post$a + post$b*(50 - xbar) #putting in 50 minus x bar plus plausible params for a and b give the dist for height around someone 50 kg
dens(mu_at_50)

# now this must be done for every weight value, not just 50. 
```

## we can use the link function to sample from the joint marginal posterior of parameters, and compute mu for each set of params.
## link samples from the posterior distribution for each case in the data
``` {r echo = TRUE}
# define sequence of weights to compute predictions for
# these values will be on the horizontal axis
weight.seq <- seq( from=25 , to=70 , by=1 )
# use link to compute mu
# for each sample from posterior
# and for each weight in weight.seq
mu <- link(lin.mod , data=data.frame(weight=weight.seq) )
str(mu)
#link returns a matrix. Rows are samples from posterior distribution and columns are cases (rows) in the data
apply(mu, 2, mean) #gives the mean of each column (2) so you have the mean height for each case (in this case the values from 25 to 70)
# use type="n" to hide raw data
plot( height ~ weight , d2 , type="n" )
# loop over samples and plot each mu value
for ( i in 1:100 )
points( weight.seq , mu[i,] , pch=16 , col=col.alpha(rangi2,0.1) )
# plots distribution of mu for each weight between 25 and 70. 

# summarize the distribution of mu
mu.mean <- apply( mu , 2 , mean ) #compute mean of each columns (2) of mu
mu.PI <- apply( mu , 2 , PI , prob=0.89 ) #cmopute 89% PI at each value

# plot raw data 4.57
# fading out points to make line and interval more visible
plot( height ~ weight , data=d2 , col=col.alpha(rangi2,0.5) )
# plot the MAP line, aka the mean mu for each weight
lines( weight.seq , mu.mean )
# plot a shaded region for 89%
shade( mu.PI , weight.seq, add = TRUE )
```
## Link by default uses the original data, so you must pass it new horizontal axis values for which to plot the posterior prediction
## the below code replicates what the function link does 
## general overview: 1) use link to generate posterior distributions for mu, pass it new horizontal axis values for you to plot the posterior prediction against. 2) use mean or PI to find summary of mu for each value of the predictor, 3) use lines and shade to plot 
```{r echo = TRUE}
post <- extract.samples(lin.mod)
mu.link <- function(weight) post$a + post$b*( weight - xbar )
weight.seq <- seq( from=25 , to=70 , by=1 )
mu <- sapply( weight.seq , mu.link )
mu.mean <- apply( mu , 2 , mean )
mu.CI <- apply( mu , 2 , PI , prob=0.89 )

```

## to get prediction intervals across all heights (not just the mean), the sigma needs to be incorporated
### sample from Gaussian distribution with mean mu and sigma samples. The line is distributed around mu, goverened by sigma
## Sim will do this by simulating heights. This does not just simulate plausible heights like link, but includes sigma
```{r echo = TRUE}
sim.height <- sim(lin.mod , data=list(weight=weight.seq) ) #simulates heights, not distributions of plausible average heights
str(sim.height)
height.PI <- apply( sim.height , 2 , PI , prob=0.89 ) #summarize these heights (get the PI for the)
mu.HPDI <- apply(sim.height, 2, HPDI, prob = 0.89) #highest desntiy posterior interval containing 89% of density

#plotting
# plot raw data 
plot( height ~ weight , d2 , col=col.alpha(rangi2,0.5) )
lines( weight.seq , mu.mean ) # draw MAP line
shade( mu.HPDI , weight.seq )# draw HPDI region for line
# draw PI region for simulated heights
shade( height.PI , weight.seq )
#This shading may be smoothed out by sampling more values from the posterior in sim
```


## polynomial regression and b-line splines
```{r echo = TRUE}
plot(d$height, d$weight) #leaving young children in results in a curved fit
#fit with second order polynomial a + B1Xi + B2Xi^2. First standardize your variables

d$weight_s <- ( d$weight - mean(d$weight) )/sd(d$weight) #standardizing variables
d$weight_s2 <- d$weight_s^2

polymod <- quap(
  alist(
      height ~ dnorm( mu , sigma ) ,
      mu <- a + b1*weight_s + b2*weight_s2 ,
      a ~ dnorm( 178 , 20 ) ,
      b1 ~ dlnorm( 0 , 1 ) ,
      b2 ~ dnorm( 0 , 1 ) ,
      sigma ~ dunif( 0 , 50 )
    ), data = d
)
precis(polymod)

```

``` {R echo = TRUE}
weight.seq <- seq( from=-2.2 , to=2 , length.out=30 ) #SEt grid of length 30 for the Z scores of weight from standardization above
pred_dat <- list( weight_s=weight.seq , weight_s2=weight.seq^2 )
mu <- link( polymod , data=pred_dat ) #generate posterior distributions for the range of weights defined above not using random sigma yet
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )
sim.height <- sim( polymod , data=pred_dat )
height.PI <- apply( sim.height , 2 , PI , prob=0.89 )

plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5) ) 
lines( weight.seq , mu.mean )
shade( mu.PI , weight.seq )
shade( height.PI , weight.seq )

# to recover the non-standardized values of weight
plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5) , xaxt="n" )
at <- c(-2,-1,0,1,2) 
labels <- at*sd(d$weight) + mean(d$weight)
axis( side=1 , at=at , labels=round(labels,1) )
lines( weight.seq , mu.mean )
shade( mu.PI , weight.seq )
shade( height.PI , weight.seq )
```

```{r echo = TRUE}
#fit a cubic term
d$weight_s3 <- d$weight^3
cubmod <- quap(
    alist(
      height ~ dnorm( mu , sigma ) ,
      mu <- a + b1*weight_s + b2*weight_s2 + b3*weight_s3 ,
      a ~ dnorm( 178 , 20 ) ,
      b1 ~ dlnorm( 0 , 1 ) ,
      b2 ~ dnorm( 0 , 10 ) ,
      b3 ~ dnorm( 0 , 10 ) ,
      sigma ~ dunif( 0 , 50 )
  ), data = d
)


precis(cubmod)

weight.seq <- seq( from=-2.2 , to=2 , length.out=30 ) #SEt grid of length 30 for the Z scores of weight from standardization above
pred_dat <- list( weight_s=weight.seq , weight_s2=weight.seq^2, weight_s3=weight.seq^3 )
mu <- link( cubmod , data=pred_dat ) #generate posterior distributions for the range of weights defined above not using random sigma yet
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )
sim.height <- sim( cubmod , data=pred_dat )
height.PI <- apply( sim.height , 2 , PI , prob=0.89 )

plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5) ) 
lines( weight.seq , mu.mean )
shade( mu.PI , weight.seq )
shade( height.PI , weight.seq )

```

## Splines
## B splines stand for basis splines, which divide the full range into parts and assign a parameter value to each part.
### This is done by creating a series of synthetic predictor variables which turn parameters on or off. Each variable is a basis function. The result is similar, MUi = alpha + w1Bi,1 + w2Bi,2 +...
### How this is done: divide the graph into pivot points or knots
```{r echo = TRUE}
data(cherry_blossoms)
d <- cherry_blossoms
precis(d)
plot(d$year, d$temp)


d2 <- d[ complete.cases(d$temp) , ] # complete cases on temp
num_knots <- 15
knot_list <- quantile( d2$year , probs=seq(0,1,length.out=num_knots) )
# R has function to build basis functions for number of knots and degree you choose.
# B will have rows for each year and the column is a basis function
library(splines)
B <- bs(d2$year,
knots=knot_list[-c(1,num_knots)] ,
degree=3 , intercept=TRUE )

plot( NULL , xlim=range(d2$year) , ylim=c(0,1) , xlab="year" , ylab="basis value")
for ( i in 1:ncol(B) ) lines( d2$year , B[,i] )
```

```{r echo = TRUE}
m4.7 <- quap(
    alist(
      T ~ dnorm( mu , sigma ) ,
      mu <- a + B %*% w ,
      a ~ dnorm(6,10),
      w ~ dnorm(0,1),
      sigma ~ dexp(1)
    ),
  data=list( T=d2$temp , B=B ) ,
  start=list( w=rep( 0 , ncol(B) ) ) )


post <- extract.samples(m4.7)
w <- apply( post$w , 2 , mean )
plot( NULL , xlim=range(d2$year) , ylim=c(-2,2) ,
xlab="year" , ylab="basis * weight" )
for ( i in 1:ncol(B) ) lines( d2$year , w[i]*B[,i] )
```
## example questions, say height is better assoc with log(weight). Fit quap with this 
```{r echo = TRUE}
d <- Howell1
logMod <- quap(
  alist(
      height ~ dnorm( mu , sigma ) ,
      mu <- a + (log(weight) - mean(log(weight))),
      a ~ dnorm( 178 , 20 ) ,
      b ~ dlnorm( 0 , 1 ),
      sigma ~ dunif( 0 , 50 )
    ), data = d
)
precis(logMod)

```

## Homework 
```{r echo = TRUE}
rm(list=ls())
data("Howell1")
d <- Howell1
d <- subset(d, d$age > 18)
xbar <- mean(d$weight)
# first calcualte the posterior using Quap
linmod <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*(weight - xbar),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0,1),
    sigma ~ dunif(0, 50)
  ), data = d
)
# want to know  posterior prediction for each case in the table
dat <- data.frame(weight = c(45, 40, 65, 31, 53))
h_sim <- sim(linmod, data = dat) #use sim instead of link to include the sigma parameter in the joint marginal posterior calls for each weight
EH <- apply(h_sim,2,mean) #calculate mean for each column of h_sim
h_ci <- apply(h_sim,2,PI) #calculate 89% PI for each columns
dat$EH <- EH
dat$L89 <- h_ci[1,]
dat$h89 <- h_ci[2,]
dat


#### number 2 ####
d <- Howell1
d$logweight <- log(d$weight)
xbar <- mean(d$logweight)

logmod <- quap(
  alist(
      height ~ dnorm( mu , sigma ) ,
      mu <- a + b1*(logweight - xbar) ,
      a ~ dnorm( 178 , 20 ) ,
      b1 ~ dlnorm( 0 , 1 ) ,
      sigma ~ dunif( 0 , 50 )
    ), data = d
)
precis(logmod)

xvec <- log(1:60)
mu <- sim(logmod, data = list(logweight=xvec))
mu_mean <- apply(mu,2,mean)
mu_ci <- apply(mu,2,PI,0.99)
plot(d$height, d$weight)
lines(exp(xvec),mu_mean)
shade(mu_ci, exp(xvec))
```

```{r echo = TRUE}
d <- Howell1
d <- subset(d, d$age < 18)

#optional, define average weight
xbar <- mean(d$weight)

lin.mod <- quap(
  flist <- alist(
    height ~ dnorm(mu, sigma), #general formula we expect, height depends on an unknown mu and sigma
    mu <- a + b*(weight- xbar), #define the linear model alpha * beta(diff_in_weight)
    a ~ dnorm(178,20), #the alpha (intercept) prior from a normal distribution with mean 178, sigma 20
    b ~ dlnorm(0,1), #beta is from a log-normal distribution between 0 and 1
    sigma ~ dunif(0,50) #uniform distribution (flat prior) for sigma
  ), data = d
)
precis(lin.mod)

weight.seq <- seq( from=4 , to=40 , by=1 )
# use sim to simulate draws from posterior
mu <- sim(lin.mod , data=data.frame(weight=weight.seq) )
# summarize the distribution of mu
mu.mean <- apply( mu , 2 , mean ) #compute mean of each columns (2) of mu
mu.PI <- apply( mu , 2 , PI , prob=0.89 ) #cmopute 89% PI at each value

plot( height ~ weight , data=d , col=col.alpha(rangi2,0.5) )
# plot the MAP line, aka the mean mu for each weight
lines( weight.seq , mu.mean )
# plot a shaded region for 89%
shade( mu.PI , weight.seq, add = TRUE )
```
